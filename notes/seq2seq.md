# Papers
- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (2014)
- Sequence to Sequence Learning with Neural Networks (2014)

# Note
- Both papers show Seq2Seq model is cool.
- LSTM can map an input sentence with variable length into a fixed-legth vector representation.
